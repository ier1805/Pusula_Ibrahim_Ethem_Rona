


# Kütüphanelerin çağırılması
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# Veri seti tanıtılması
pd.set_option("display.max_columns", None)
pd.set_option("display.width", 500)
df = pd.read_excel("Talent_Academy_Case_DT_2025.xlsx")


df.head()


# Veri setinin kopyası oluşturma
df2 = df.copy()


# Veri setine ait bilgiler
df2.info()


# Veri setiyle ilgili bilgiler
def check_df(dataframe, head = 5):
    print("############### Shape ###############")
    print(df2.shape)
    print("############### N/A ###############")
    print(df2.isnull().sum())
check_df(df2)


# Kategorik Sütunların Belirtilmesi
cat_cols = [col for col in df2.columns if str(df2[col].dtypes) in ["category","object","bool"]]
num_but_cat = [col for col in df2.columns if df2[col].nunique() < 10 and df2[col].dtypes in ["int","float"]]
cat_but_car = [col for col in df2.columns if df[col].nunique() > 20 and df2[col].dtypes in ["category","object"]]
cat_cols = cat_cols + num_but_cat


# Kategorik sütunların frekans analizi
for index, value in df2[cat_cols].items():
    print(value.value_counts(),"-"*50, sep="\n")


# Numerik sütunların getirilmesi
num_cols = [col for col in df2.columns if df2[col].dtypes in ["int","float"]]


# Numerik sütunların frekans analizi
for index, value in df2[num_cols].items():
    print(value.value_counts(),"-"*50, sep="\n")





df2.head()


# Tedavi Süresi ve Uygulama Süresinin İnteger Dönüştürülmesi
for col in ["TedaviSuresi","UygulamaSuresi"]:
    df2[col] = df2[col].str.extract("(\d+)").astype("int64")


df2[["TedaviSuresi","UygulamaSuresi"]]


# Kategorik Sütunların Belirtilmesi
cat_cols = [col for col in df2.columns if str(df2[col].dtypes) in ["category","object","bool"]]
num_but_cat = [col for col in df2.columns if df2[col].nunique() < 10 and df2[col].dtypes in ["int","float"]]
cat_but_car = [col for col in df2.columns if df[col].nunique() > 20 and df2[col].dtypes in ["category","object"]]
cat_cols = cat_cols + num_but_cat


cat_cols


cat_but_car


# Numerik sütunların getirilmesi
num_cols = [col for col in df2.columns if df2[col].dtypes in ["int","float"]]


num_cols


# Kategorik sütunlarda benzersiz değerler
df2[cat_cols].nunique()


# Numerik sütunlarda benzersiz değerler
df2[num_cols].nunique()


# Veri setindeki belirli sütunların aykırılıklarının tespiti
# Not: Aykırılıklarda ekleme-çıkarma yapılmamıştır.
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))

ax1.boxplot(pd.to_numeric(df2["Yas"], errors="coerce").dropna())
ax1.set_title("Yaş")

ax2.boxplot(pd.to_numeric(df2["TedaviSuresi"], errors="coerce").dropna())
ax2.set_title("Tedavi Süresi")

ax3.boxplot(pd.to_numeric(df2["UygulamaSuresi"], errors="coerce").dropna())
ax3.set_title("Uygulama Süresi")

plt.tight_layout()
plt.show()





# Eksik ve boş verilerin tahminsel olarak doldurulması
# Not: Değerler diğer parametre değerlerini baz alıp eksiklikleri tahmine göre dolduracaktır.
import pandas as pd
import numpy as np

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

uygulamaSuresi = "UygulamaSuresi"
minTrain = 10  # Modele geçmek için minimum bilinen etiket sayısı

# Yardımcılar
def normalize_labels(series, target):
    """
    Açıklama: Bu kısımda cinsiyetleri tek tipe indirgedik.

    Avantajlar:
    
        1. Kadın, Erkek varyantlarını tek sınıfta topluyoruz. Hem value_counts/groupby sonuçları doğru olur hem de OneHotEncoder gereksiz ekstra sınıf üretmez. 
        2. Boş stringler veya yazıyla Nan değerler gerçek Nan yapılır. Aksi halde bunlar modelde ayrı bir kategoriye dönüşüp hatalı özellikler yaratır. İmputation çalışmaz.
        3. Her Çalışmada aynı girdiden aynı çıktıyı üretir. Deterministik hale getirir.
    Özet: Bu adım veri ön işleme hattında temiz, tutarlı ve veri-model uyumlu etiketler için zorunlu bir standartlaştırma 
    """
    s = series.astype(str).str.strip()
    if target == "Cinsiyet":
        gmap = {"k": "Kadın", "kadın": "Kadın", "kadin": "Kadın", "bayan": "Kadın",
                "e": "Erkek", "erkek": "Erkek", "bay": "Erkek"}
        return s.str.lower().map(gmap)
    # Boşluk ya da "nan"/"NaN" vb. → NaN
    return s.replace({r"^\s*$": np.nan, r"(?i)^\s*nan\s*$": np.nan}, regex=True)

def build_feature_frame(df_, target):
    """
    Açıklama: Modelleme/Imputasyon için özellik Matrisi(X) hazırlanır. Sayısal/Kategorik/Metin alanlarını derler. Hedef sızıntıyı önler.

    Avantajlar:

        1. KanGrubu, Uyruk, Bolum, UygulamaYerleri, TedaviAdi sütunlarını doğrudan etkiler.
        2. KronikHastalik, Alerji, Tanilar kolonlarını birleştirip tek bir TEXT alanı yapar.
        3. Yalnızca mevcut kolonları ekler, yoksa atlar.
    Amaç: Dağınık veriyi tek bir yerde toplayıp, sayısalları temizlemek, kategorikleri ayırmak, metni birleştirerek modele uyumlu bir (matrisX) oluşturmak.
    """
    X = pd.DataFrame(index=df_.index)

    # Sayısallar
    if "Yas" in df_:
        X["Yas"] = pd.to_numeric(df_["Yas"], errors="coerce")
    if "TedaviSuresi" in df_:
        X["TS"] = pd.to_numeric(df_["TedaviSuresi"].astype(str).str.extract(r"(\d+)")[0], errors="coerce")
    if uygulamaSuresi in df_:
        X["US"] = pd.to_numeric(df_[uygulamaSuresi], errors="coerce")

    # Kategorikler (hedef hariç; sızıntı önleme)
    for cat in ["KanGrubu", "Uyruk", "Bolum", "UygulamaYerleri", "TedaviAdi"]:
        if cat in df_.columns and cat != target:
            X[cat] = df_[cat]

    # Metin (hedef hariç)
    text_cands = ["KronikHastalik", "Alerji", "Tanilar"]
    text_cols = [c for c in text_cands if c in df_.columns and c != target]
    if text_cols:
        X["TEXT"] = df_[text_cols].fillna("").agg(" ".join, axis=1)

    return X

def groupwise_mode_fill(df_, target, m_missing, groupings):
    """
    Açıklama: Hangi satırları doldurabileceğimizi seçiyoruz.
    
    m_missing: hedef sütun (target) eksik olan satırları (True/False maske)

    Amaç: to_fill, group moduyla doldurulmaya uygun satırları işaretler, hedef değer yok ama ait olduğu grubu (örn: Bolum, Uyruk) belirlemek için eksiksiz olarak gereken alanları
    """
    y_all = df_[target]
    for keys in groupings:
        keys = [k for k in (keys if isinstance(keys, (list, tuple)) else [keys]) if k in df_.columns] # Listeye çevirir zaten list/tuple ise olduğu gibi kullanır.
        if not keys: # Geçerli hiçbir kolon kalmadıysa atlar.
            continue

        grp = (df_.loc[~y_all.isna(), keys + [target]] # Hedefi eksik olmayan satırları alır
                 .groupby(keys)[target] # Keys üzerinden gruplar. 
                 .agg(lambda x: x.mode().iat[0] if len(x.mode()) else np.nan)) # Her grup için mode hesaplanır.

        to_fill = m_missing & ~df_[keys].isna().any(axis=1) # Hedefi eksik (m_missing == True) ve tüm grup anahtarları dolu olan (~ ... any(axis=1)) satırlar.
        if to_fill.any():  # Bu gruplamayla doldurulabilecek en az bir satır varsa, doldurma adımına geçer.
            vals = df_.loc[to_fill, keys].merge(grp.reset_index(), on=keys, how="left")[target].values
            new_vals = pd.Series(vals, index=df_.index[to_fill]) # Bulunan mod değerlerini, orijinal indeksle hizalı bir Seri’ye çevirir.
            mask_new = to_fill & new_vals.notna() # Gerçekten mod bulunabilen (notna) satırları işaretler.
            df_.loc[mask_new, target] = new_vals[mask_new] # Hedef sütundaki eksik değerleri, bulunan grup modu ile yerine yazar.

            # Eksik maskeyi güncelle
            m_missing = df_[target].isna() | df_[target].astype(str).str.strip().eq("") # Doldurmadan sonra eksik maskeyi yeniden hesaplar.
            if not m_missing.any(): # Hedefte artık hiç eksik kalmadıysa, daha fazla farklı gruplama denemeye gerek yok, döngüden çıkar.
                break
    return df_, m_missing

def fill_silent(df_, target, min_train: int = minTrain):
    """
    Açıklama: Yalnızca eksik olanları doldurur: Model -> Grup modası -> Genel moda.
    min_train burada görünmese de genelde model eğitmek için gereken asgari örnek sayısı eşiğidir; fonksiyonun ilerleyen kısmında kullanılır.
    
    Amaç: Bu adımda, fonksiyonun erken çıkış koşullarını kurar: hedef sütun yoksa, eksik yoksa veya öğrenilecek örnek yoksa gereksiz hesap yapmaz.
    """
    if target not in df_.columns:
        return df_

    # Standartlaştırma ve eksik maskesi
    df_[target] = normalize_labels(df_[target], target)
    m_missing = df_[target].isna() | df_[target].astype(str).str.strip().eq("")
    if not m_missing.any():
        return df_

    known = ~m_missing & df_[target].notna()
    if known.sum() == 0:
        return df_

    # Özellikler
    X = build_feature_frame(df_, target) # Özellik matrisX’i (sayısal/kategorik/metin sütunlarını derler; hedef sızıntısını önler)
    num_cols = [c for c in ["Yas", "TS", "US"] if c in X.columns] # matrisX’te mevcutsa sayısal sütunları listele: Yas, TS= TedaviSuresi (sayısallaştırılmış), US= UygulamaSuresi
    cat_cols = [c for c in ["KanGrubu", "Uyruk", "Bolum", "UygulamaYerleri", "TedaviAdi"] if c in X.columns] # matrisX’te mevcutsa kategorik sütunları listele
    # Metin özelliği var mı?
    # 1) "TEXT" kolonu X’te var mı? (yoksa kısa devre yapıp KeyError’dan kaçınır)
    # 2) TEXT içinde, kırpılmış hali boş olmayan en az bir satır var mı?
    has_text = ("TEXT" in X.columns) and X["TEXT"].astype(str).str.strip().ne("").any()
    # En az bir türden özellik var mı? (sayısal veya kategorik listeleri boş değilse ya da metin varsa)
    # Model eğitimi için yeter şartlar sağlanıyor mu?
    # - known.sum() >= min_train  → eğitim için yeter sayıda etiketli örnek var mı?
    # - df_.loc[known, target].nunique() >= 2 → hedefte en az iki farklı sınıf/değer var mı? (tek sınıfsa model öğrenemez)
    # - has_features → giriş özelliği var mı? (özellik yoksa model kurmanın anlamı yok)
    has_features = bool(num_cols or cat_cols or has_text)

    # Model için yeter şartlar
    can_train = (known.sum() >= min_train) and (df_.loc[known, target].nunique() >= 2) and has_features

    # (1) Model tabanlı doldurma
    if can_train:
        transformers = [
            ("num", SimpleImputer(strategy="median"), num_cols),
            ("cat", Pipeline([
                ("imp", SimpleImputer(strategy="most_frequent")),
                ("oh", OneHotEncoder(handle_unknown="ignore"))
            ]), cat_cols),
        ]
        if has_text:
            # ColumnTransformer TEXT'i 2D geçirir; TF-IDF 1D ister -> squeeze ile düzleştiriyoruz.
            txt_pipe = Pipeline([
                ("squeeze", FunctionTransformer(lambda a: np.ravel(a), validate=False)),
                ("tfidf", TfidfVectorizer(min_df=3, max_features=2000))
            ])
            transformers.append(("txt", txt_pipe, "TEXT"))

        pre = ColumnTransformer(transformers, remainder="drop", sparse_threshold=0.3)
        model = Pipeline([
            ("pre", pre),
            ("clf", LogisticRegression(max_iter=1000, class_weight="balanced"))
        ])

        model.fit(X.loc[known], df_.loc[known, target])
        df_.loc[m_missing, target] = model.predict(X.loc[m_missing])

        # Eksik maskesini güncelle
        m_missing = df_[target].isna() | df_[target].astype(str).str.strip().eq("")

    # (2) Grup modası (hedefe özel strateji)
    if m_missing.any():
        groupings_map = {
            "Tanilar": [("Bolum", "TedaviAdi"), ("TedaviAdi",), ("Bolum",)],
            "UygulamaYerleri": [("Bolum", "TedaviAdi"), ("Bolum",)],
            "KanGrubu": [("Uyruk",), ("Bolum",)],
            "Cinsiyet": [("Uyruk",), ("Bolum",)],
            "Bolum": [("TedaviAdi", "UygulamaYerleri"), ("TedaviAdi",), ("UygulamaYerleri",), ("Cinsiyet",)],
        }
        groupings = groupings_map.get(target, [("Bolum", "TedaviAdi"), ("Bolum",)])
        df_, m_missing = groupwise_mode_fill(df_, target, m_missing, groupings)

    # (3) Genel moda
    if m_missing.any():
        mode_val = df_.loc[known, target].mode().iat[0]
        df_.loc[m_missing, target] = mode_val

    return df_

# Sadece eksiklikleri doldurma
for tgt in ["Cinsiyet", "UygulamaYerleri", "KanGrubu", "Tanilar", "Bolum"]:
    if tgt in df2.columns:  
        df2 = fill_silent(df2, tgt, min_train=minTrain)

# Temizle & Kaydet
obj_cols = df2.select_dtypes(include="object").columns
df2[obj_cols] = df2[obj_cols].replace({r"(?i)^\s*nan\s*$": ""}, regex=True)

df2.to_excel("veri.xlsx", index=False, na_rep="")
print("Dosya kaydedildi: veri.xlsx")


df2.head()


# KronikHastalik & Alerji: boşları "Belirtilmemiştir" ile doldurulması
for col in ["KronikHastalik", "Alerji"]:
    if col in df2.columns:
        df2[col] = (
            df2[col]
            .replace({r"^\s*$": np.nan, r"^\s*nan\s*$": np.nan}, regex=True)
            .fillna("Belirtilmemiştir")
        )

# Temizlik + Kaydet (veri.xlsx)
obj_cols = df2.select_dtypes(include="object").columns
df2[obj_cols] = df2[obj_cols].replace({r"^\s*nan\s*$": ""}, regex=True)

df2.to_excel("veri.xlsx", index=False, na_rep="")
print("Dosya kaydedildi: veri.xlsx")


check_df(df2)


# Veri Setinin istatistiksel özeti
df2.describe().T


# Veri setinin kategorik değerlein istatistiksel özeti


df2.describe(include = "object").T





import re, unicodedata
import numpy as np, pandas as pd
from collections import defaultdict
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import KFold, GroupKFold
from sklearn.pipeline import Pipeline

# ----------------- Yardımcılar -----------------
def _norm(s):
    s = "" if s is None else str(s)
    s = s.strip().casefold()
    s = unicodedata.normalize('NFC', s)
    s = re.sub(r'\s+', ' ', s)
    return s

def _tokens(x, sep=r'[,\|;/]+'):
    if x is None or (isinstance(x, float) and np.isnan(x)):
        return []
    if not isinstance(x, str):
        return []
    return [t.strip() for t in re.split(sep, _norm(x)) if t.strip()]

def _reduce(vals, how='mean'):
    if len(vals) == 0: return np.nan
    if how == 'max':    return float(np.max(vals))
    if how == 'min':    return float(np.min(vals))
    if how == 'median': return float(np.median(vals))
    return float(np.mean(vals))

def _to_float_series(s):
    s = s.astype(str).str.replace('\xa0',' ', regex=False).str.strip()
    s = s.str.replace(',', '.', regex=False)
    s = s.str.extract(r'([-+]?\d+(?:\.\d+)?)', expand=False)
    return pd.to_numeric(s, errors='coerce')

# ---- Ham metni saklamak için cache (yedek kolon oluşturmadan tekrar koşmaya izin verir)
RAW_CACHE = {}
def _get_raw_view(X, cols):
    out = {}
    for c in cols:
        ser = X[c]
        if ser.dtype == 'object':
            RAW_CACHE[c] = ser.copy()                  # ilk çalıştırmada ham metni kaydet
            out[c] = ser
        else:
            out[c] = RAW_CACHE.get(c, pd.Series([""]*len(X), index=X.index)).reindex(X.index)
    return pd.DataFrame(out, index=X.index)

def reset_raw_cache():
    RAW_CACHE.clear()

# ----------------- OOF Target Encoder -----------------
class OOFMultiTargetEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, cols, n_splits=5, smoothing_k=10.0, sep=r'[,\|;/]+', agg='mean', random_state=42):
        self.cols = list(cols); self.n_splits = n_splits
        self.k = float(smoothing_k); self.sep = sep
        self.agg = agg; self.random_state = random_state
        self.mean_all_, self.global_mean_ = {}, None

    def _build_full_mapping(self, X_raw, y):
        y = pd.Series(y).astype(float); mask = y.notna(); yv = y[mask]
        self.global_mean_ = float(yv.mean()) if len(yv) else 0.0
        self.mean_all_.clear()
        for c in self.cols:
            sums, cnts = defaultdict(float), defaultdict(int)
            for i, val in X_raw[c][mask].items():
                yi = yv.loc[i]
                for t in _tokens(val, self.sep):
                    sums[t] += float(yi); cnts[t] += 1
            self.mean_all_[c] = {t: (sums[t] + self.k*self.global_mean_) / (cnt + self.k)
                                 for t, cnt in cnts.items()}

    def fit(self, X, y):
        X_raw = _get_raw_view(X, self.cols)
        self._build_full_mapping(X_raw, y)
        return self

    def fit_transform(self, X, y, groups=None):
        X = X.copy(); y = pd.Series(y).astype(float)
        X_raw = _get_raw_view(X, self.cols)

        if groups is not None:
            splitter = GroupKFold(self.n_splits).split(X_raw, y, groups=groups)
        else:
            splitter = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state).split(X_raw)

        out = {c: pd.Series(index=X.index, dtype=float) for c in self.cols}
        for tr, te in splitter:
            y_tr = y.iloc[tr]; gmean = float(y_tr.mean()) if len(y_tr) else 0.0
            for c in self.cols:
                sums, cnts = defaultdict(float), defaultdict(int)
                for i, val in X_raw[c].iloc[tr].items():
                    yi = y.loc[i]
                    if pd.isna(yi): continue
                    for t in _tokens(val, self.sep):
                        sums[t] += float(yi); cnts[t] += 1
                means = {t: (sums[t] + self.k*gmean) / (cnt + self.k) for t, cnt in cnts.items()}

                vals = []
                for i, val in X_raw[c].iloc[te].items():
                    toks = _tokens(val, self.sep)
                    vals.append(gmean if not toks else _reduce([means.get(t, gmean) for t in toks], self.agg))
                out[c].iloc[te] = vals

        X_oof = pd.DataFrame({c: out[c].astype(float) for c in self.cols}, index=X.index)
        self._build_full_mapping(X_raw, y)
        return X_oof

    def transform(self, X):
        X = X.copy(); X_raw = _get_raw_view(X, self.cols); gmean = self.global_mean_
        X_enc = pd.DataFrame(index=X.index)
        for c in self.cols:
            means = self.mean_all_.get(c, {})
            X_enc[c] = X_raw[c].apply(
                lambda v: gmean if len(_tokens(v, self.sep))==0
                else _reduce([means.get(t, gmean) for t in _tokens(v, self.sep)], self.agg)
            )
        return X_enc

# ----------------- KULLANIM (ekstra kolon OLUŞTURMADAN) -----------------
# 0) Eski _ORIG kolonlar kaldıysa temizle (tek seferlik)
df2.drop(columns=[c for c in df2.columns if c.endswith("_ORIG")], inplace=True, errors="ignore")

# 1) Hedef
target = _to_float_series(df2["TedaviSuresi"])

# 2) Encode edilecek metin kolonları
candidates = ["KronikHastalik","Alerji","Tanilar","TedaviAdi","UygulamaYerleri","Uygulama Yerleri","Bolum"]
cols = [c for c in candidates if c in df2.columns]

# 3) Encoder
enc = OOFMultiTargetEncoder(cols=cols, n_splits=5, smoothing_k=10.0, sep=r'[,\|;/]+', agg='mean', random_state=42)

# 4) OOF fit_transform (HastaNo varsa gruplu)
groups = df2["HastaNo"] if "HastaNo" in df2.columns else None
df_te = enc.fit_transform(df2[cols], y=target, groups=groups)

# 5) Aynı kolonların içine yaz (EKSTRA kolon yok)
if len(cols) > 0:
    df2.loc[:, cols] = df_te.values



df2.head()





# Label ve OneHotEncoding dönüştürme
import re
import pandas as pd

def encode_cinsiyet_uyruk_kangrubu(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()

    # -------- Cinsiyet -> stabil integer --------
    if "Cinsiyet" in out.columns:
        gmap = {"k":"Kadın","kadın":"Kadın","kadin":"Kadın","bayan":"Kadın",
                "e":"Erkek","erkek":"Erkek","bay":"Erkek"}
        s = (out["Cinsiyet"].astype(str).str.strip().str.lower().map(gmap))
        s = s.fillna("Unknown")
        cmap = {"Kadın":0, "Erkek":1, "Unknown":2}    # sabit harita
        out["Cinsiyet"] = s.map(cmap).astype("int8")

    # -------- Uyruk & KanGrubu -> One-Hot --------
    for col, prefix in (("Uyruk","Uyruk_"), ("KanGrubu","KanGrubu_")):
        if col in out.columns:
            # Eski dummy'leri temizle (yeniden çalıştırmada yığılmayı önler)
            old_dummy_cols = [c for c in out.columns if c.startswith(prefix)]
            if old_dummy_cols:
                out.drop(columns=old_dummy_cols, inplace=True)

            s = (out[col].astype(str).str.strip()
                            .replace({r"^\s*$":"Unknown", r"^\s*nan\s*$":"Unknown"}, regex=True))
            dummies = pd.get_dummies(s, prefix=col, dtype="uint8")
            out = pd.concat([out.drop(columns=[col]), dummies], axis=1)

    return out

df2 = encode_cinsiyet_uyruk_kangrubu(df2)


df2.head()





import numpy as np, pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.linear_model import RidgeCV
from sklearn.model_selection import GroupKFold, cross_validate

TARGET, ID = "TedaviSuresi", "HastaNo"

X = df2.drop(columns=[TARGET])
y = df2[TARGET].astype(float).to_numpy()
groups = df2[ID] if ID in df2.columns else None

# 1) Sayısal kolonlar
num_cols_all = X.select_dtypes(include=np.number).columns.tolist()

# 2) 0/1 olan one-hot binariler -> ölçekleme
bin_cols = [c for c in num_cols_all
            if set(pd.unique(X[c].dropna())) <= {0, 1}]

# 3) Zaten z-skor'a benzeyenler (μ≈0, σ≈1) -> tekrar ölçekleme
stats = X[num_cols_all].agg(['mean', 'std']).T
pre_scaled = stats.index[
    (stats['mean'].abs() < 0.15) & (stats['std'].between(0.85, 1.15))
].tolist()

# 4) Cinsiyet'i ordinal bırakmayalım; one-hot'a alacağız
do_not_scale = set([ID, "Cinsiyet"]) | set(bin_cols) | set(pre_scaled)
scale_cols = [c for c in num_cols_all if c not in do_not_scale]

# 5) Kategorikler (sende çoğu dummy'lenmiş; Cinsiyet'i özellikle ekliyoruz)
cat_cols = ["Cinsiyet"]  # 0/1/2 değerleri varsa da one-hot yapar

pre = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), scale_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols),
    ],
    remainder="passthrough"
)

model = Pipeline([
    ("prep", pre),
    ("est", RidgeCV(alphas=np.logspace(-3, 3, 13)))
])

cv = GroupKFold(n_splits=5) if groups is not None else 5
scoring = {"mae":"neg_mean_absolute_error","rmse":"neg_root_mean_squared_error","r2":"r2"}

res = cross_validate(model, X, y,
                     cv=cv.split(X, y, groups) if groups is not None else cv,
                     scoring=scoring, n_jobs=-1)

print("Ridge:",
      {m.split('_',1)[1].upper():
           (-res[m].mean() if "neg_" in scoring.get(m.split('_',1)[1], "") else res[m].mean())
       for m in res if m.startswith("test_")})

# Karşılaştırma için aptal-baz RMSE (sürekli ortalama tahmini):
rmse_null = y.std(ddof=0)
print("Null RMSE (mean baseline) ≈", rmse_null)

